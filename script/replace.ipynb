{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "651c44f5f6e341b4892655e626bcbc79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"/data/yimin/models/base/meta-llama/Meta-Llama-3-8B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_imdb = \"/data/yimin/peft/peft_trainer/output/IMDB_9800\"\n",
    "adapter_legal = \"/data/yimin/peft/peft_trainer/output/LEGAL_9800\"\n",
    "adapter_medqa = \"/data/yimin/peft/peft_trainer/output/MedQA_9800\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_list = ['legal', 'medqa', 'imdb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load time:0.29088521003723145\n",
      "load time:0.07834386825561523\n",
      "load time:0.08379936218261719\n"
     ]
    }
   ],
   "source": [
    "for adapter in adapter_list:\n",
    "    start_load_time = time.time()\n",
    "    model.load_adapter(locals()[f'adapter_{adapter}'], adapter_name=f'{adapter}_adapter')\n",
    "    end_load_time = time.time()\n",
    "    print(f\"load time:{end_load_time - start_load_time}\")\n",
    "# start_load_time = time.time()\n",
    "# model.load_adapter(adapter_imdb, adapter_name=f'adapter4')\n",
    "# end_load_time = time.time()\n",
    "# print(f\"load time:{end_load_time - start_load_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "/home/tx/miniconda3/envs/backend/lib/python3.10/site-packages/transformers/generation/utils.py:1127: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set time:0.004769802093505859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate time:8.385138034820557\n",
      "Hello to you!\"<<endl); //prints hello to you!//askfortheratingofthe\n",
      "set time:0.00467991828918457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate time:7.7810585498809814\n",
      "Hello; Hi, what’s your name? - I’m Bill.\n",
      "3. What’s your name\n",
      "set time:0.005828380584716797\n",
      "generate time:3.675081729888916\n",
      "Hello  World!'\n",
      "\\end{code}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "text = \"Hello\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to('cuda')\n",
    "for adapter in adapter_list:\n",
    "    start_set_time = time.time()\n",
    "    model.set_adapter(f\"{adapter}_adapter\")\n",
    "    end_set_time = time.time()\n",
    "    print(f\"set time:{end_set_time - start_set_time}\")\n",
    "    output = model.generate(**inputs, do_sample=True)\n",
    "    end_generate_time = time.time()\n",
    "    print(f\"generate time:{end_generate_time - end_set_time}\")\n",
    "    print(tokenizer.decode(output[0], skip_special_tokens=True))   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "backend",
   "language": "python",
   "name": "backend"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
